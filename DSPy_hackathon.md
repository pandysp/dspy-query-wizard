# Project: From Guesswork to Engineering

### _A/B Testing Human Intuition vs. Automated Optimization_

## 1. The Problem (The "Why")

Building AI apps today feels less like engineering and more like "voodoo." When an AI fails to answer a question, we spend hours manually tweaking promptsâ€”changing adjectives, adding "please," or asking it to "think step-by-step."

- It is unscientific.

- It is unscalable.

- It breaks whenever the model changes.

## 2. The Solution (The "What")

We are building aÂ **Comparative RAG System**Â that pits a human against a machine.

- **Contestant A (Human):**Â Uses standard, manually written prompts (e.g., "Search for X").

- **Contestant B (DSPy):**Â Uses an automated optimizer that "compiles" prompts by learning from data, mathematically maximizing retrieval accuracy.

**The Goal:**Â A live dashboard proving that automated prompt engineering outperforms human intuition on complex, multi-hop questions.

---

## 3. The Architecture

We are keeping this lean to move fast.

- **The Engine:**Â **DSPy**Â (Python framework for programming LLMs).

- **The Data:**Â **HotPotQA**Â (A pre-existing dataset of "hard" multi-step questions) +Â **Wikipedia**.

  - _Note:_Â We areÂ **NOT**Â scraping our own data. We are connecting to a public, pre-indexed Wikipedia server to save time.

- **The Backend:**Â **FastAPI**. Serves the logic and runs the comparison.

- **The Frontend:**Â **React**. Visualizes the battle.

---

## 4. The Strategy (Minimal Effort, High Impact)

To ensure we finish on time, we are adhering to these constraints:

1. **No Custom Indexing:**Â We use the public ColBERTv2/Wikipedia index.

2. **Small Training Set:**Â We will train the optimizer on just 20â€“50 examples from the HotPotQA dataset.

3. **Optimization Target:**Â We are strictly optimizingÂ **Query Rephrasing**.

    - _Human approach:_Â Searches for the user's question literally.

    - _Machine approach:_Â Breaks the question down into sub-queries or hypothetical answers to find better documents.

---

## 5. Roles & Responsibilities

### ðŸŽ¨ Frontend (React)

**Goal:**Â Visualize the "Man vs. Machine" comparison.

- **The View:**Â A split-screen interface. Left side = Human results. Right side = Machine results.

- **The Data:**Â Display theÂ **Input Question**, theÂ **Retrieved Context**Â (the text chunks found), and theÂ **Final Answer**.

- **The "Cool Factor":**Â Show theÂ _internal thought process_Â (the "Prompt Evolution") of the Machine side. Show usÂ _how_Â it rewrote the query.

- **The Scoreboard:**Â A simple metric bar showing success/fail for the current query.

### âš™ï¸ Backend (FastAPI + DSPy)

**Goal:**Â Build the logic pipeline.

- **Integration:**Â Connect DSPy to the OpenAI API and the public ColBERTv2 retriever.

- **The "Human" Route:**Â Build a standard RAG chain (Retrieve -> Generate).

- **The "Machine" Route:**Â Build the DSPy module and run theÂ `BootstrapFewShot`Â optimizer to "compile" the prompt using the training set.

- **Endpoints:**Â Expose a simple API that takes aÂ `question`Â and returns both Human and Machine answers + metadata.

### ðŸ§  Data & Pitch Lead (The Narrative)

**Goal:**Â Ensure the demo doesn't fail.

- **Curate the "Evil" Questions:**Â Select 5-10 specific questions from the dataset where weÂ _know_Â the simple search fails but the optimized search succeeds.

- **The Narrative:**Â Prepare the script explainingÂ _why_Â the machine won (e.g., "Notice how the machine broke the question into two parts, while the human prompt got stuck on keywords").

---

## 6. Timeline

1. **Hour 1:**Â Everyone agrees on the API JSON structure (Input/Output).

2. **Hour 2-3:**

    - Backend: Get the DSPy optimizer running and save the "compiled" program.

    - Frontend: Skeleton of the split-screen UI.

3. **Hour 4:**Â Integration. Connect React to FastAPI.

4. **Hour 5:**Â Polish the "Evil Questions" list and style the UI to make the "Winner" obvious.
